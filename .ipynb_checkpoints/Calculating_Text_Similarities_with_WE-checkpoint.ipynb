{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Using Pretrained Word Embeddings to Find Word Similarity \n",
    "  \n",
    "Problem: You need to find out whether two words are similar but not equal.  \n",
    "  \n",
    "Solution: Use a pretrained word embedding model. In the code example we use `genism`, a Python library for topic modeling.  \n",
    "\n",
    "First step is to aquire a pretrained model, here we use Google News. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikkibytes/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from keras.utils import get_file\n",
    "import gensim\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(10, 10)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import json\n",
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'GoogleNews-vectors-negative300.bin'\n",
    "path = get_file(MODEL + '.gz', 'https://deeplearning4jblob.blob.core.windows.net/resources/wordvectors/%s.gz' % MODEL)\n",
    "if not os.path.isdir('generated'):\n",
    "    os.mkdir('generated')\n",
    "\n",
    "unzipped = os.path.join('generated', MODEL)\n",
    "if not os.path.isfile(unzipped):\n",
    "    with open(unzipped, 'wb') as fout:\n",
    "        zcat = subprocess.Popen(['zcat'],\n",
    "                          stdin=open(path),\n",
    "                          stdout=fout\n",
    "                         )\n",
    "        zcat.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(unzipped, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once model has finished loading we can use it to find similar words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cappuccino', 0.6888186931610107),\n",
       " ('mocha', 0.6686208844184875),\n",
       " ('coffee', 0.6616826057434082),\n",
       " ('latte', 0.6536753177642822),\n",
       " ('caramel_macchiato', 0.6491268873214722),\n",
       " ('ristretto', 0.6485545635223389),\n",
       " ('espressos', 0.6438628435134888),\n",
       " ('macchiato', 0.6428250074386597),\n",
       " ('chai_latte', 0.6308028697967529),\n",
       " ('espresso_cappuccino', 0.6280542612075806)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['espresso'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion: Word embeddings associate an n-dimensional vector with each word in the vocabulary in a way that similar words are near each other. Finding similar words is a mere **nearest-neighbor** search. The Word2vec embeddings are obtained by training a NN to predict a word from its context. Words that can be inserted into similar patterns will get vectors that are close to each other. Here we don't care about the actual task, just about the assigned weights, which we get as a side effect of training the network.  \n",
    "  \n",
    "Note later we will see how word embeddings can also be used to feed words into a neural network. It is much more feasible to feed a 300-dim embedding vector into a network than a 3-million-dim one that is one-hot encoded. Moreover, a network fed with pretrained word embeddings doesn't have to learn the relationships between the wrods, but can start with the real task at hand immediately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Word2vec Math  \n",
    "\n",
    "#### Problem: How can you automatically answer questions of the form \"A is to B as C is to what?\"  \n",
    "  \n",
    "#### Solution: Use the semantic properties of the Word2vec model. The `gensim` library makes this straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_is_to_B_as_C_is_to(a, b, c, topn=1):\n",
    "    a, b, c = map(lambda x:x if type(x) == list else [x], (a, b, c))\n",
    "    res = model.most_similar(positive=b + c, negative=a, topn=topn)\n",
    "    if len(res):\n",
    "        if topn == 1:\n",
    "            return res[0][0]\n",
    "        return [x[0] for x in res]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply this to arbitrary words--for example to find what relates to \"king\" the way \"son\" relates to \"daughter\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_is_to_B_as_C_is_to('man', 'woman', 'king')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approached can also be used to look up the capitals of selected countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rome is the capital of Italy\n",
      "Paris is the capital of France\n",
      "Delhi is the capital of India\n",
      "Beijing is the capital of China\n"
     ]
    }
   ],
   "source": [
    "for country in 'Italy', 'France', 'India', 'China':\n",
    "    print('%s is the capital of %s' % \n",
    "          (A_is_to_B_as_C_is_to('Germany', 'Berlin', country), country))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can do the same for important products for given companies. Here we seed the products equation with two products, the iPhone for Apple and Starbucks_coffee for Starbucks. Note that numbers are replaced by # in the embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google -> personalized_homepage, app, Gmail\n",
      "IBM -> DB2, WebSphere_Portal, Tamino_XML_Server\n",
      "Boeing -> Dreamliner, airframe, aircraft\n",
      "Microsoft -> Windows_Mobile, SyncMate, Windows\n",
      "Samsung -> MM_A###, handset, Samsung_SCH_B###\n"
     ]
    }
   ],
   "source": [
    "for company in 'Google', 'IBM', 'Boeing', 'Microsoft', 'Samsung':\n",
    "    products = A_is_to_B_as_C_is_to(\n",
    "        ['Starbucks', 'Apple'], \n",
    "        ['Starbucks_coffee', 'iPhone'], \n",
    "        company, topn=3)\n",
    "    print('%s -> %s' % \n",
    "          (company, ', '.join(products)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** As we have seen words that are similar to each other have vectors that are close to eacg other. It turns out that the difference between the word vector also encodes the difference between words. The `most_similar` method takes one or more positive words and one or more negative words. It looks up the corresponding vectors, then deducts the negative from the positive and returns the words that have vectors nearest to the resulting vector.  \n",
    "  \n",
    "To answer the question \"A is to B as C is to?\" we want to deduct A from B and then add C, or call `most_similar` with positive = [B,C] and negative = [A]. The example A_is_to_B_as_C_is_to adds two small features to this behavior. If we request only one example, it will return a single item, rather than a list with one item. We can also return either lists or single items for A,B and C.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
