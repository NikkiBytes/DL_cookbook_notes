{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Using Pretrained Word Embeddings to Find Word Similarity \n",
    "  \n",
    "Problem: You need to find out whether two words are similar but not equal.  \n",
    "  \n",
    "Solution: Use a pretrained word embedding model. In the code example we use `genism`, a Python library for topic modeling.  \n",
    "\n",
    "First step is to aquire a pretrained model, here we use Google News. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikkibytes/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from keras.utils import get_file\n",
    "import gensim\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(10, 10)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import json\n",
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'GoogleNews-vectors-negative300.bin'\n",
    "path = get_file(MODEL + '.gz', 'https://deeplearning4jblob.blob.core.windows.net/resources/wordvectors/%s.gz' % MODEL)\n",
    "if not os.path.isdir('generated'):\n",
    "    os.mkdir('generated')\n",
    "\n",
    "unzipped = os.path.join('generated', MODEL)\n",
    "if not os.path.isfile(unzipped):\n",
    "    with open(unzipped, 'wb') as fout:\n",
    "        zcat = subprocess.Popen(['zcat'],\n",
    "                          stdin=open(path),\n",
    "                          stdout=fout\n",
    "                         )\n",
    "        zcat.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(unzipped, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once model has finished loading we can use it to find similar words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cappuccino', 0.6888186931610107),\n",
       " ('mocha', 0.6686208844184875),\n",
       " ('coffee', 0.6616826057434082),\n",
       " ('latte', 0.6536753177642822),\n",
       " ('caramel_macchiato', 0.6491268873214722),\n",
       " ('ristretto', 0.6485545635223389),\n",
       " ('espressos', 0.6438628435134888),\n",
       " ('macchiato', 0.6428250074386597),\n",
       " ('chai_latte', 0.6308028697967529),\n",
       " ('espresso_cappuccino', 0.6280542612075806)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['espresso'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion: Word embeddings associate an n-dimensional vector with each word in the vocabulary in a way that similar words are near each other. Finding similar words is a mere **nearest-neighbor** search. The Word2vec embeddings are obtained by training a NN to predict a word from its context. Words that can be inserted into similar patterns will get vectors that are close to each other. Here we don't care about the actual task, just about the assigned weights, which we get as a side effect of training the network.  \n",
    "  \n",
    "Note later we will see how word embeddings can also be used to feed words into a neural network. It is much more feasible to feed a 300-dim embedding vector into a network than a 3-million-dim one that is one-hot encoded. Moreover, a network fed with pretrained word embeddings doesn't have to learn the relationships between the wrods, but can start with the real task at hand immediately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Word2vec Math  \n",
    "\n",
    "#### Problem: How can you automatically answer questions of the form \"A is to B as C is to what?\"  \n",
    "  \n",
    "#### Solution: Use the semantic properties of the Word2vec model. The `gensim` library makes this straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
